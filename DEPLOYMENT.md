# Despliegue de AMA-Intent v3

## üéØ Prop√≥sito del Proyecto

AMA-Intent v3 est√° dise√±ado para **ejecuci√≥n local** usando Ollama con modelos de lenguaje locales. Este proyecto NO est√° dise√±ado para despliegue en servicios serverless como Vercel o AWS Lambda.

## ‚úÖ Despliegue Local (Recomendado)

Este es el m√©todo recomendado y soportado:

### Requisitos
- Python 3.10 o superior
- Ollama instalado y ejecut√°ndose
- Sistema operativo: Linux, macOS, o Windows

### Instalaci√≥n

```bash
# 1. Clonar el repositorio
git clone https://github.com/dgr198213-ui/proyecto-ama-intent.git
cd proyecto-ama-intent

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Configurar variables de entorno (opcional)
cp .env.example .env
# Editar .env seg√∫n tus necesidades

# 4. Iniciar Ollama (en otra terminal)
ollama serve

# 5. Iniciar el servidor
python start.py
```

El sistema estar√° disponible en `http://localhost:5001`

## ‚ùå Despliegue en Vercel/Serverless NO Soportado

AMA-Intent v3 **NO** es compatible con Vercel u otros servicios serverless porque:

1. **Requiere Ollama**: El sistema necesita una instancia de Ollama ejecut√°ndose localmente
2. **Sin Supabase**: La versi√≥n actual solo usa SQLite local para persistencia
3. **Arquitectura Local**: Dise√±ado espec√≠ficamente para ejecuci√≥n en m√°quinas locales

### ¬øPor qu√© no Vercel?

- Ollama no puede ejecutarse en entornos serverless
- El filesystem de Vercel es de solo lectura (excepto /tmp que es ef√≠mero)
- El modelo de ejecuci√≥n serverless no es compatible con servicios de AI locales

## üê≥ Despliegue con Docker (Experimental)

Aunque existe un Dockerfile, ten en cuenta que:

```bash
# Construir la imagen
docker build -t ama-intent .

# Ejecutar (requiere acceso a Ollama)
docker run -p 5001:5001 \
  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \
  -v $(pwd)/data:/app/data \
  ama-intent
```

**Nota**: Ollama debe estar ejecut√°ndose en el host y accesible desde el contenedor.

## üîß Configuraci√≥n

### Variables de Entorno Esenciales

```bash
# Servidor
HOST=127.0.0.1        # Local only
PORT=5001             # Puerto del servidor

# Seguridad
AMA_SHARED_SECRET=tu-secreto-aqui  # Para autenticaci√≥n

# Ollama
OLLAMA_MODEL=llama3.1  # Modelo a usar

# Memoria
MEMORY_CONTEXT_LIMIT=5     # Contexto reciente
MEMORY_MAX_ENTRIES=1000    # M√°ximo de entradas
MEMORY_ARCHIVE_DAYS=30     # D√≠as antes de archivar
```

## üìä Base de Datos

El sistema usa **SQLite** para persistencia local:
- Ubicaci√≥n: `data/ama_memory.db`
- Autom√°ticamente creado en el primer inicio
- Backups recomendados de la carpeta `data/`

## üöÄ Producci√≥n Local

Para ejecutar en producci√≥n local:

```bash
# 1. Configurar secreto fuerte
export AMA_SHARED_SECRET=$(openssl rand -hex 32)

# 2. Ejecutar sin auto-reload
export RELOAD=false

# 3. Iniciar
python start.py
```

## üîê Seguridad

- ‚úÖ El servidor se ejecuta en `127.0.0.1` por defecto (solo accesible localmente)
- ‚úÖ Usa `AMA_SHARED_SECRET` para proteger endpoints sensibles
- ‚úÖ Los datos se almacenan localmente en SQLite
- ‚ùå NO exponer el puerto 5001 a internet sin autenticaci√≥n adicional

## üìù Notas

- Este proyecto prioriza simplicidad y ejecuci√≥n local
- No hay planes actuales para soportar despliegue serverless
- Para uso empresarial, considera ejecutar en un servidor dedicado

## üÜò Soporte

Para problemas con el despliegue, abre un issue en GitHub describiendo:
- Sistema operativo
- Versi√≥n de Python
- Versi√≥n de Ollama
- Logs de error completos
