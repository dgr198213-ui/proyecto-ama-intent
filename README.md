# ğŸ§  Cerebro Artificial + IA Local Gobernada

## GuÃ­a Completa de InstalaciÃ³n y Uso

---

## ğŸ“‹ Tabla de Contenidos

1. [DescripciÃ³n del Sistema](#descripciÃ³n)
2. [Requisitos](#requisitos)
3. [InstalaciÃ³n](#instalaciÃ³n)
4. [ConfiguraciÃ³n](#configuraciÃ³n)
5. [Uso BÃ¡sico](#uso-bÃ¡sico)
6. [Comandos Avanzados](#comandos-avanzados)
7. [Estructura del Proyecto](#estructura)
8. [Troubleshooting](#troubleshooting)

---

## ğŸ¯ DescripciÃ³n del Sistema {#descripciÃ³n}

Este sistema integra un **Cerebro Artificial biomimÃ©tico** con **LLMs locales** (Ollama/LM Studio) para crear una IA completamente gobernada que:

âœ… **Filtra y valida** cada entrada del usuario (AMA-G Fase 1)
âœ… **AÃ±ade contexto de seguridad** sin modificar el prompt (Shadow Prompt)
âœ… **Valida cada respuesta** contra la intenciÃ³n original (AMA-G Fase 3)
âœ… **Aprende continuamente** de cada interacciÃ³n
âœ… **Consolida memoria** mediante ciclos de "sueÃ±o"
âœ… **Se auto-regula** con homeostasis PID

### Arquitectura del Sistema

```
Usuario â†’ AMA-Intent â†’ Cerebro (Fase 1) â†’ Shadow Prompt 
    â†“
Ollama/LM Studio (GeneraciÃ³n)
    â†“
Cerebro (Fase 3 ValidaciÃ³n) â†’ ConsolidaciÃ³n â†’ Usuario
```

---

## ğŸ’» Requisitos {#requisitos}

### Hardware MÃ­nimo

- **RAM**: 8 GB (16 GB recomendado)
- **Almacenamiento**: 10 GB libres
- **GPU** (opcional): Acelera modelos locales

### Software

- **Python**: 3.8 o superior
- **Ollama** O **LM Studio**: Motor de IA local
- **Sistema Operativo**: Windows 10/11, macOS, Linux

---

## ğŸ“¦ InstalaciÃ³n {#instalaciÃ³n}

### Paso 1: Instalar Python

**Windows:**
```bash
# Descargar desde python.org
# Marcar "Add Python to PATH" durante instalaciÃ³n
python --version  # Verificar
```

**macOS/Linux:**
```bash
# Python suele venir preinstalado
python3 --version
```

### Paso 2: Instalar Ollama O LM Studio

#### OpciÃ³n A: Ollama (Recomendado)

**Windows/macOS/Linux:**
```bash
# Descargar desde https://ollama.ai
# Instalar ejecutable

# Verificar instalaciÃ³n
ollama --version

# Descargar modelo (ejemplo: Gemma 2B)
ollama pull gemma2:2b

# Otros modelos disponibles:
# ollama pull qwen2.5:3b
# ollama pull llama3.2:3b
# ollama pull mistral:7b
```

#### OpciÃ³n B: LM Studio

1. Descargar desde: https://lmstudio.ai
2. Instalar aplicaciÃ³n
3. Descargar modelos desde la interfaz
4. Iniciar servidor local (puerto 1234 por defecto)

### Paso 3: Instalar Dependencias Python

```bash
# Crear entorno virtual (recomendado)
python -m venv venv

# Activar entorno
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Instalar dependencias
pip install numpy requests

# Opcional (mejora embeddings):
pip install sentence-transformers scikit-learn
```

### Paso 4: Descargar el CÃ³digo del Cerebro

```bash
# Crear directorio del proyecto
mkdir cerebro_artificial
cd cerebro_artificial

# Copiar todos los archivos .py del proyecto:
# - sensing/kalman.py
# - cortex/attention.py
# - cortex/state.py
# - memory/episodic_graph.py
# - memory/semantic_matrix.py
# - memory/working_memory.py
# - memory/pruning.py
# - decision/q_value.py
# - decision/dmd.py
# - governance/amag_audit.py
# - control/pid_homeostasis.py
# - learning/loss.py
# - learning/stability.py
# - learning/consolidation.py
# - ama_intent.py
# - brain_complete.py
# - ollama_brain_interface.py
# - cli_interactive.py
```

---

## âš™ï¸ ConfiguraciÃ³n {#configuraciÃ³n}

### ConfiguraciÃ³n de Ollama

```bash
# AsegÃºrate de que Ollama estÃ© ejecutÃ¡ndose
ollama serve  # Si no se iniciÃ³ automÃ¡ticamente

# Verificar modelos instalados
ollama list

# El servidor corre en: http://localhost:11434
```

### ConfiguraciÃ³n de LM Studio

1. Abrir LM Studio
2. Ir a "Local Server"
3. Click en "Start Server"
4. Copiar la URL (normalmente `http://localhost:1234`)

### Variables de Entorno (Opcional)

```bash
# Windows (PowerShell):
$env:OLLAMA_HOST = "http://localhost:11434"

# macOS/Linux:
export OLLAMA_HOST="http://localhost:11434"
```

---

## ğŸš€ Uso BÃ¡sico {#uso-bÃ¡sico}

### Iniciar el Sistema

```bash
# Activar entorno virtual (si lo creaste)
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Ejecutar CLI interactiva
python cli_interactive.py
```

### Primera ConfiguraciÃ³n

Al iniciar, el sistema te preguntarÃ¡:

```
ConfiguraciÃ³n del LLM:
  Provider (ollama/lmstudio) [ollama]: ollama
  URL [http://localhost:11434]: [Enter para default]
  Modelo [gemma2:2b]: [Enter o escribe tu modelo]
  Temperature [0.7]: [Enter para default]
```

### Ejemplo de ConversaciÃ³n

```
ğŸ§‘ TÃº: Â¿QuÃ© es la inteligencia artificial?

[El sistema procesa...]

ğŸ¤– IA: La inteligencia artificial (IA) es...

   â±ï¸  Tiempo: 3.45s
   âœ“ Confianza: 0.82
```

### Comandos Especiales

Durante el chat, puedes usar:

```
/help      - Muestra ayuda
/stats     - EstadÃ­sticas del sistema
/config    - Muestra configuraciÃ³n
/sleep     - Fuerza consolidaciÃ³n
/history   - Historial de conversaciÃ³n
/export    - Exporta estadÃ­sticas a JSON
/exit      - Salir
```

---

## ğŸ”§ Comandos Avanzados {#comandos-avanzados}

### Ver EstadÃ­sticas Detalladas

```
ğŸ§‘ TÃº: /stats

ğŸ“Š ESTADÃSTICAS DEL SISTEMA

ğŸ’¬ ConversaciÃ³n:
  Total de mensajes: 15
  Tasa de aprobaciÃ³n: 93.3%

ğŸ§  Cerebro:
  Ticks ejecutados: 30
  Episodios en memoria: 25
  Conceptos semÃ¡nticos: 8
  Ciclos de sueÃ±o: 1

ğŸ›¡ï¸ Gobernanza (AMA-G):
  AuditorÃ­as totales: 30
  Tasa de aprobaciÃ³n: 90.0%
  Tasa de revisiÃ³n: 6.7%
  Tasa de fallo: 3.3%
```

### Forzar Ciclo de SueÃ±o

```
ğŸ§‘ TÃº: /sleep

ğŸ’¤ Iniciando ciclo de sueÃ±o forzado...

[FASE 1/4] NREM - ConsolidaciÃ³n sistemÃ¡tica...
[FASE 2/4] REM - Procesamiento creativo...
[FASE 3/4] ReorganizaciÃ³n de memoria...
[FASE 4/4] Homeostasis y preparaciÃ³n...

âœ… Ciclo de sueÃ±o completado
  Episodios consolidados: 150
  Conceptos fusionados: 2
  Items podados: 5
```

### Exportar Datos

```
ğŸ§‘ TÃº: /export

âœ… EstadÃ­sticas exportadas a: brain_stats_20250103_143022.json
```

---

## ğŸ“ Estructura del Proyecto {#estructura}

```
cerebro_artificial/
â”‚
â”œâ”€â”€ sensing/
â”‚   â””â”€â”€ kalman.py              # Filtro talÃ¡mico (Kalman)
â”‚
â”œâ”€â”€ cortex/
â”‚   â”œâ”€â”€ attention.py           # AtenciÃ³n cortical (LSI)
â”‚   â””â”€â”€ state.py               # Estado latente
â”‚
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ episodic_graph.py      # Memoria episÃ³dica (PageRank)
â”‚   â”œâ”€â”€ semantic_matrix.py     # Memoria semÃ¡ntica
â”‚   â”œâ”€â”€ working_memory.py      # Working memory (PFC)
â”‚   â””â”€â”€ pruning.py             # Sistema de poda
â”‚
â”œâ”€â”€ decision/
â”‚   â”œâ”€â”€ q_value.py             # EvaluaciÃ³n Q (MIEM)
â”‚   â””â”€â”€ dmd.py                 # DecisiÃ³n matricial
â”‚
â”œâ”€â”€ governance/
â”‚   â””â”€â”€ amag_audit.py          # Auditor AMA-G
â”‚
â”œâ”€â”€ control/
â”‚   â””â”€â”€ pid_homeostasis.py     # Control PID homeostÃ¡tico
â”‚
â”œâ”€â”€ learning/
â”‚   â”œâ”€â”€ loss.py                # FunciÃ³n de pÃ©rdida
â”‚   â”œâ”€â”€ stability.py           # Control de estabilidad
â”‚   â””â”€â”€ consolidation.py       # Ciclo de sueÃ±o
â”‚
â”œâ”€â”€ ama_intent.py              # Extractor de intenciÃ³n
â”œâ”€â”€ brain_complete.py          # Cerebro integrado
â”œâ”€â”€ ollama_brain_interface.py # Interfaz con LLM
â”œâ”€â”€ cli_interactive.py         # CLI interactiva
â”‚
â””â”€â”€ venv/                      # Entorno virtual
```

---

## ğŸ” Troubleshooting {#troubleshooting}

### Problema: "No se puede conectar a Ollama"

**SoluciÃ³n:**
```bash
# Verificar que Ollama estÃ© ejecutÃ¡ndose
ollama serve

# Verificar puerto
curl http://localhost:11434/api/tags

# Si usa otro puerto, especificarlo:
# En cli_interactive.py, cambiar base_url
```

### Problema: "Modelo no encontrado"

**SoluciÃ³n:**
```bash
# Listar modelos instalados
ollama list

# Descargar modelo necesario
ollama pull gemma2:2b
```

### Problema: "Out of Memory" o lentitud

**SoluciÃ³n:**
- Usar modelos mÃ¡s pequeÃ±os (2B-3B en lugar de 7B-13B)
- Reducir `max_episodes` y `max_concepts` en `brain_config`
- Cerrar otras aplicaciones

```python
# En cli_interactive.py, lÃ­nea ~150:
brain_config = CompleteBrainConfig(
    max_episodes=500,      # Reducir de 1000
    max_concepts=100,      # Reducir de 200
    sleep_interval=100     # Aumentar intervalo
)
```

### Problema: "ModuleNotFoundError"

**SoluciÃ³n:**
```bash
# Asegurarse de estar en el entorno virtual
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate     # Windows

# Reinstalar dependencias
pip install numpy requests
```

### Problema: Respuestas siempre bloqueadas

**SoluciÃ³n:**
- Revisar umbrales de AMA-G
- Ver logs de auditorÃ­a con `/stats`
- Ajustar `GovernanceThresholds`:

```python
# En ollama_brain_interface.py:
self.auditor = AMAGAuditor(
    thresholds=GovernanceThresholds(
        min_confidence=0.3,    # Reducir de 0.5
        max_surprise=5.0,      # Aumentar de 3.0
        max_risk=0.9           # Aumentar de 0.7
    )
)
```

---

## ğŸ“š Recursos Adicionales

### Modelos Recomendados

| Modelo | TamaÃ±o | RAM Requerida | Velocidad | Calidad |
|--------|--------|---------------|-----------|---------|
| gemma2:2b | 2B | 4 GB | âš¡âš¡âš¡ | â­â­â­ |
| qwen2.5:3b | 3B | 6 GB | âš¡âš¡ | â­â­â­â­ |
| llama3.2:3b | 3B | 6 GB | âš¡âš¡ | â­â­â­â­ |
| mistral:7b | 7B | 12 GB | âš¡ | â­â­â­â­â­ |

### Enlaces Ãštiles

- **Ollama**: https://ollama.ai
- **LM Studio**: https://lmstudio.ai
- **Modelos disponibles**: https://ollama.ai/library

---

## ğŸ“ PrÃ³ximos Pasos

1. âœ… **Instalar y probar** el sistema bÃ¡sico
2. âœ… **Experimentar** con diferentes modelos
3. âœ… **Ajustar parÃ¡metros** de gobernanza segÃºn necesidad
4. ğŸ”œ **Personalizar** para casos de uso especÃ­ficos
5. ğŸ”œ **Integrar** con otras aplicaciones

---

## ğŸ“ Notas de VersiÃ³n

**v1.0.0** - VersiÃ³n inicial completa
- FASE 1: PercepciÃ³n + DecisiÃ³n + Gobernanza âœ…
- FASE 2: Sistema de Memoria completo âœ…
- FASE 3: Aprendizaje + Homeostasis âœ…
- IntegraciÃ³n con Ollama/LM Studio âœ…
- CLI interactiva âœ…

---

## ğŸ¤ Soporte

Â¿Problemas o preguntas?
- Revisa la secciÃ³n **Troubleshooting**
- Verifica que todos los mÃ³dulos estÃ©n instalados correctamente
- AsegÃºrate de que Ollama/LM Studio estÃ© ejecutÃ¡ndose

---

**Â¡Disfruta de tu IA local gobernada!** ğŸ§ ğŸš€