# üß† AMA-Intent v3: Cerebro Local Biomim√©tico

Sistema de inteligencia artificial biomim√©tica dise√±ado para ejecutar procesos de manera local, funcionando como "Cortex" de Qodeia.com. 

Esta versi√≥n v3 representa una refactorizaci√≥n completa hacia una arquitectura minimalista y funcional, reduciendo las dependencias en un 84% y simplificando la estructura en un 80%.

## üöÄ Funcionalidad
- **Local**: Corre completamente en tu m√°quina usando Ollama con Llama 3.1
- **Inteligente**: Memoria SQLite persistente con clasificaci√≥n de intenci√≥n autom√°tica
- **Conectado**: HTTP API FastHTML para integraci√≥n con aplicaciones externas
- **Seguro**: Ejecuci√≥n localhost por defecto, sin exposici√≥n a internet

## üìÅ Estructura del Proyecto

```plaintext
proyecto-ama-intent/
‚îú‚îÄ‚îÄ .env                  # (NO SUBIR A GITHUB) Claves y secretos
‚îú‚îÄ‚îÄ .gitignore            # Importante: para ignorar .env y __pycache__
‚îú‚îÄ‚îÄ README.md             # El manual de uso biomim√©tico
‚îú‚îÄ‚îÄ requirements.txt      # Dependencias ligeras
‚îú‚îÄ‚îÄ start.py              # El √∫nico archivo que necesitas ejecutar
‚îú‚îÄ‚îÄ data/                 # Donde vive tu memoria (SQLite)
‚îÇ   ‚îî‚îÄ‚îÄ ama_memory.db
‚îú‚îÄ‚îÄ local_cortex/         # üß† L√ìGICA PURA (Tu cerebro local)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ thought.py        # Procesa texto con Llama 3
‚îÇ   ‚îî‚îÄ‚îÄ memory.py         # Gestiona recuerdos en SQLite
‚îî‚îÄ‚îÄ bridge/               # üåâ CONEXI√ìN (Servidor Web)
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îî‚îÄ‚îÄ server.py         # API FastHTML que habla con Qodeia.com
```

## üõ†Ô∏è Instalaci√≥n

### Requisitos Previos
1. Python 3.8 o superior
2. Ollama instalado y corriendo

### Pasos de Instalaci√≥n

```bash
# 1. Clonar el repositorio
git clone https://github.com/dgr198213-ui/proyecto-ama-intent.git
cd proyecto-ama-intent

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Verificar que Ollama est√° corriendo
ollama serve  # En otra terminal

# 4. Descargar el modelo (si no lo tienes)
ollama pull llama3.1
```

## üöÄ Uso

### Iniciar el Sistema

```bash
python start.py
```

El sistema:
1. Verificar√° la carpeta `data/` (la crear√° si no existe)
2. Verificar√° que Ollama est√° disponible
3. Iniciar√° el servidor en puerto 5001

### Acceder a la Interfaz

Abre tu navegador en: http://localhost:5001

### Acceder al Panel de Administraci√≥n

Para ver estad√≠sticas y gestionar el sistema: http://localhost:5001/admin

### Panel de Gesti√≥n

Para gestionar credenciales: http://localhost:5001/credenciales

### API Endpoints

**GET** `/api/health` (Nuevo en v3.2)

Endpoint de health check con autenticaci√≥n.

**Headers requeridos:**
- `X-AMA-Secret`: El secreto compartido configurado en `AMA_SHARED_SECRET`

**Respuesta:**
```json
{
  "status": "healthy",
  "timestamp": "2026-01-23T20:35:16.064640",
  "memory_stats": {
    "total_interactions": 0,
    "by_intent": {},
    "first_interaction": null,
    "last_interaction": null
  },
  "security_warnings": [
    "‚ÑπÔ∏è FERNET_KEY no configurado (opcional)"
  ]
}
```

**POST** `/api/synapse` (Requiere autenticaci√≥n en v3.2)

Endpoint principal que procesa solicitudes.

**Headers requeridos:**
- `X-AMA-Secret`: El secreto compartido configurado en `AMA_SHARED_SECRET`

**Par√°metros:**
- `input` (string): El texto a procesar

**Respuesta:**
```json
{
  "status": "success",
  "intent": "CHAT|CODIGO|ANALISIS",
  "confidence": 0.8,
  "response": "Respuesta generada por el modelo",
  "timestamp": "2026-01-23T16:35:20.123456"
}
```

**GET** `/api/memory/search?q={query}&limit={limit}`

Busca en la memoria del sistema.

**Par√°metros:**
- `q` (string): T√©rmino de b√∫squeda
- `limit` (int, opcional): N√∫mero m√°ximo de resultados (default: 10)

**Respuesta:**
```json
{
  "status": "success",
  "query": "Python",
  "count": 3,
  "results": [
    {
      "timestamp": "2026-01-23T16:35:20.123456",
      "input": "What is Python?",
      "output": "Python is...",
      "intent": "CHAT"
    }
  ]
}
```

**GET** `/api/memory/stats`

Obtiene estad√≠sticas de la memoria del sistema.

**Respuesta:**
```json
{
  "status": "success",
  "stats": {
    "total_interactions": 150,
    "by_intent": {
      "CHAT": 80,
      "CODIGO": 50,
      "ANALISIS": 20
    },
    "first_interaction": "2026-01-20T10:00:00",
    "last_interaction": "2026-01-23T16:35:20"
  }
}
```

**POST** `/api/memory/cleanup`

Limpia pensamientos antiguos de la memoria.

**Par√°metros:**
- `days` (int, opcional): D√≠as de antig√ºedad para limpiar (default: 30)

**Respuesta:**
```json
{
  "status": "success",
  "deleted_count": 25,
  "message": "Cleaned up 25 thoughts older than 30 days"
}
```

**GET** `/api/memory/by-intent/{intent}`

Obtiene pensamientos filtrados por tipo de intenci√≥n.

**Par√°metros:**
- `intent` (string): CHAT, CODIGO, o ANALISIS
- `limit` (int, opcional): N√∫mero m√°ximo de resultados (default: 10)

**Respuesta:**
```json
{
  "status": "success",
  "intent": "CODIGO",
  "count": 10,
  "results": [
    {
      "timestamp": "2026-01-23T16:35:20",
      "input": "Write a function",
      "output": "def example(): ..."
    }
  ]
}
```

## üß† Arquitectura

### Local Cortex (Cerebro Local)
- **thought.py**: Procesa entradas usando Llama 3.1 a trav√©s de Ollama
  - `LocalBrain.think()`: Genera respuestas contextualizadas
  - `LocalBrain.fast_classify()`: Clasifica el tipo de solicitud

- **memory.py**: Gestiona la memoria persistente
  - `init_db()`: Inicializa la base de datos SQLite
  - `save_thought()`: Guarda interacciones
  - `get_last_thoughts()`: Recupera contexto reciente

### Bridge (Puente HTTP)
- **server.py**: API FastHTML que conecta con el mundo exterior
  - Endpoint `/`: Interfaz de monitoreo
  - Endpoint `/api/synapse`: Procesa solicitudes

## üîß Configuraci√≥n

### Variables de Entorno (.env)

```bash
# Server Configuration
HOST=127.0.0.1      # Server binding (localhost for security)
PORT=5001           # Server port
RELOAD=false        # Auto-reload (dev only)

# Security Configuration (Doctrina Howard) - NEW in v3.2
AMA_SHARED_SECRET=change-this-secret-in-production  # Shared secret for bridge authentication
FERNET_KEY=         # Optional: Leave empty to auto-generate, or provide a valid Fernet key

# Ollama Configuration
OLLAMA_MODEL=llama3.1  # LLM model to use

# Memory Configuration
MEMORY_CONTEXT_LIMIT=5     # Number of recent thoughts to use as context
MEMORY_MAX_ENTRIES=1000    # Maximum entries before triggering cleanup
MEMORY_ARCHIVE_DAYS=30     # Archive thoughts older than N days

# Logging Configuration
LOG_LEVEL=INFO  # DEBUG, INFO, WARNING, ERROR, CRITICAL
```

### üîê Gesti√≥n de Credenciales (v3.2)

AMA-Intent v3.2 incluye un panel web para gestionar credenciales de forma segura:

1. **Acceder al Panel**: Navega a `http://localhost:5001/credenciales`
2. **Editar Claves**: Modifica `AMA_SHARED_SECRET`, `FERNET_KEY`, u `OLLAMA_MODEL`
3. **Hot Reload**: Los cambios se aplican inmediatamente sin reiniciar el servidor
4. **Validaci√≥n**: El sistema valida autom√°ticamente el formato de las claves

**Nota de Seguridad**: El panel est√° dise√±ado para entornos locales protegidos. Configura adecuadamente `AMA_SHARED_SECRET` en producci√≥n.

## üìä Base de Datos

El sistema usa SQLite para persistir interacciones:

**Tabla: interactions**
- `id`: INTEGER PRIMARY KEY
- `timestamp`: TEXT (ISO 8601)
- `input`: TEXT (entrada del usuario)
- `output`: TEXT (respuesta del sistema)
- `intent`: TEXT (clasificaci√≥n: CODIGO, CHAT, ANALISIS)

## üêõ Soluci√≥n de Problemas

### "Ollama no parece estar instalado"
Aseg√∫rate de que Ollama est√° instalado y corriendo:
```bash
ollama serve
```

### "Error al conectar con Ollama"
Verifica que el modelo est√° descargado:
```bash
ollama pull llama3.1
```

### Puerto 5001 en uso
Cambia el puerto en `bridge/server.py` o usa la variable de entorno `PORT`:
```bash
PORT=5002 python start.py
```

### Error al importar m√≥dulos
Si ves errores de importaci√≥n, reinstala las dependencias:
```bash
pip install -r requirements.txt --force-reinstall
```

## üß™ Pruebas

Para verificar que todo funciona correctamente, ejecuta la suite de pruebas:

```bash
python test_ama_v3.py
```

Esta suite verifica:
- Estructura de directorios
- Importaci√≥n de m√≥dulos
- Dependencias correctas
- Sintaxis de Python
- Funciones de memoria (init, save, retrieve)
- B√∫squeda en memoria
- Estad√≠sticas de memoria
- Limpieza de memoria
- Filtrado por intenci√≥n

### Tests Actuales: 11/11 ‚úÖ

## üìã Novedades en v3

### Cambios Principales desde v2
- ‚úÖ **Reducci√≥n de dependencias**: De 25+ paquetes a solo 4
- ‚úÖ **Simplificaci√≥n estructural**: De 15+ directorios a 3 m√≥dulos core
- ‚úÖ **C√≥digo m√°s limpio**: ~200 l√≠neas vs ~10,000 l√≠neas anteriores
- ‚úÖ **Seguridad mejorada**: Localhost por defecto, context managers, subprocess seguro
- ‚úÖ **Tests automatizados**: Suite completa con 11 tests (100% cobertura core)
- ‚úÖ **API expandida**: 6 endpoints para gesti√≥n completa del sistema
- ‚úÖ **Panel de administraci√≥n**: Dashboard web para monitoreo
- ‚úÖ **B√∫squeda en memoria**: Encuentra interacciones previas por palabras clave
- ‚úÖ **Gesti√≥n autom√°tica**: Limpieza de memoria antigua
- ‚úÖ **Configuraci√≥n flexible**: Variables de entorno para personalizaci√≥n

### Nuevas Caracter√≠sticas en v3.1
- ‚úÖ **Sistema de b√∫squeda**: Busca en memoria hist√≥rica
- ‚úÖ **Estad√≠sticas avanzadas**: An√°lisis de uso por tipo de intenci√≥n
- ‚úÖ **Limpieza autom√°tica**: Gesti√≥n de memoria con archivado
- ‚úÖ **Filtros por intenci√≥n**: Recupera interacciones espec√≠ficas
- ‚úÖ **Panel de admin**: Interfaz web para monitoreo del sistema
- ‚úÖ **Mejor manejo de errores**: C√≥digos HTTP apropiados y logging
- ‚úÖ **Confidence scoring**: Las clasificaciones incluyen nivel de confianza

### Nuevas Caracter√≠sticas en v3.2 - Doctrina Howard (Producci√≥n)
- ‚úÖ **Panel de Gesti√≥n de Credenciales**: Interfaz minimalista en `/credenciales` para editar claves cr√≠ticas
- ‚úÖ **Hot Reload de Configuraci√≥n**: Recarga inmediata de variables .env sin reiniciar el servidor
- ‚úÖ **Blindaje del T√∫nel Seguro**: Validaci√≥n de secreto compartido (`AMA_SHARED_SECRET`) en endpoints cr√≠ticos
- ‚úÖ **Health Check Seguro**: Endpoint `/api/health` con autenticaci√≥n para monitoreo
- ‚úÖ **Validaci√≥n de Credenciales**: Verificaci√≥n autom√°tica de formato de claves (Fernet)
- ‚úÖ **Dashboard con Advertencias**: Panel de admin muestra alertas de seguridad en tiempo real
- ‚úÖ **Sin Frameworks Adicionales**: Implementaci√≥n pura con FastHTML, sin dependencias extras

### Caracter√≠sticas Eliminadas
- ‚ùå Dashboard web complejo
- ‚ùå Sistema de plugins v2
- ‚ùå Autenticaci√≥n multi-usuario
- ‚ùå Integraci√≥n MiniMax
- ‚ùå Soporte Docker (por ahora)

Ver `REFACTORING_SUMMARY.md` para detalles completos.

## üöÄ Despliegue en Vercel

AMA-Intent v3.2 incluye soporte para despliegue en plataformas serverless como Vercel:

### Archivos de Despliegue

- **asgi.py**: Punto de entrada ASGI para despliegue serverless
- **vercel.json**: Configuraci√≥n de Vercel para el despliegue

### Configuraci√≥n para Vercel

1. **Crea una cuenta en Vercel** (si no tienes una)
2. **Conecta tu repositorio** a Vercel
3. **Configura las variables de entorno** en el dashboard de Vercel:
   - `AMA_SHARED_SECRET`: Tu secreto compartido
   - `FERNET_KEY`: Clave de encriptaci√≥n (opcional)
   - `OLLAMA_MODEL`: Modelo a utilizar (default: llama3.1)
   
4. **Despliega**: Vercel detectar√° autom√°ticamente la configuraci√≥n

**Nota**: Para usar Ollama en Vercel, necesitar√°s configurar un endpoint externo de Ollama accesible desde internet, ya que Vercel no permite procesos persistentes.

## üéØ Pr√≥ximos Pasos

- Integraci√≥n con interfaces web externas
- Soporte para m√∫ltiples modelos
- Sistema de plugins expandible
- An√°lisis de c√≥digo avanzado

## üìû Soporte

Para reportar problemas o contribuir, abre un issue en el repositorio.

---

**AMA-Intent v3** - Sistema de Inteligencia Biomim√©tica Local
