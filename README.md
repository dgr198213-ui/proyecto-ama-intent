# ğŸ§  AMA-Intent v3: Cerebro Local BiomimÃ©tico

Sistema diseÃ±ado para ejecutar procesos como "Cortex" de Qodeia.com.

## ğŸš€ Funcionalidad
- **Local**: Corre localmente usando Ollama Llama 3.1.
- **Inteligente**: Usa memoria SQLite y clasificaciÃ³n de intenciÃ³n rÃ¡pida.
- **Conectado**: HTTP API FastHTML accesible globalmente.

## ğŸ“ Estructura del Proyecto

```plaintext
proyecto-ama-intent/
â”œâ”€â”€ .env                  # (NO SUBIR A GITHUB) Claves y secretos
â”œâ”€â”€ .gitignore            # Importante: para ignorar .env y __pycache__
â”œâ”€â”€ README.md             # El manual de uso biomimÃ©tico
â”œâ”€â”€ requirements.txt      # Dependencias ligeras
â”œâ”€â”€ start.py              # El Ãºnico archivo que necesitas ejecutar
â”œâ”€â”€ data/                 # Donde vive tu memoria (SQLite)
â”‚   â””â”€â”€ ama_memory.db
â”œâ”€â”€ local_cortex/         # ğŸ§  LÃ“GICA PURA (Tu cerebro local)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ thought.py        # Procesa texto con Llama 3
â”‚   â””â”€â”€ memory.py         # Gestiona recuerdos en SQLite
â””â”€â”€ bridge/               # ğŸŒ‰ CONEXIÃ“N (Servidor Web)
    â”œâ”€â”€ __init__.py
    â””â”€â”€ server.py         # API FastHTML que habla con Qodeia.com
```

## ğŸ› ï¸ InstalaciÃ³n

### Requisitos Previos
1. Python 3.8 o superior
2. Ollama instalado y corriendo

### Pasos de InstalaciÃ³n

```bash
# 1. Clonar el repositorio
git clone https://github.com/dgr198213-ui/proyecto-ama-intent.git
cd proyecto-ama-intent

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Verificar que Ollama estÃ¡ corriendo
ollama serve  # En otra terminal

# 4. Descargar el modelo (si no lo tienes)
ollama pull llama3.1
```

## ğŸš€ Uso

### Iniciar el Sistema

```bash
python start.py
```

El sistema:
1. VerificarÃ¡ la carpeta `data/` (la crearÃ¡ si no existe)
2. VerificarÃ¡ que Ollama estÃ¡ disponible
3. IniciarÃ¡ el servidor en puerto 5001

### Acceder a la Interfaz

Abre tu navegador en: http://localhost:5001

### API Endpoint

**POST** `/api/synapse`

**ParÃ¡metros:**
- `input` (string): El texto a procesar

**Respuesta:**
```json
{
  "status": "success",
  "intent": "CHAT|CODIGO|ANALISIS",
  "response": "Respuesta generada por el modelo",
  "timestamp": "2026-01-23T16:35:20.123456"
}
```

## ğŸ§  Arquitectura

### Local Cortex (Cerebro Local)
- **thought.py**: Procesa entradas usando Llama 3.1 a travÃ©s de Ollama
  - `LocalBrain.think()`: Genera respuestas contextualizadas
  - `LocalBrain.fast_classify()`: Clasifica el tipo de solicitud

- **memory.py**: Gestiona la memoria persistente
  - `init_db()`: Inicializa la base de datos SQLite
  - `save_thought()`: Guarda interacciones
  - `get_last_thoughts()`: Recupera contexto reciente

### Bridge (Puente HTTP)
- **server.py**: API FastHTML que conecta con el mundo exterior
  - Endpoint `/`: Interfaz de monitoreo
  - Endpoint `/api/synapse`: Procesa solicitudes

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno (.env)

```bash
# Opcional: configurar modelo diferente
OLLAMA_MODEL=llama3.1

# Opcional: cambiar puerto
PORT=5001
```

## ğŸ“Š Base de Datos

El sistema usa SQLite para persistir interacciones:

**Tabla: interactions**
- `id`: INTEGER PRIMARY KEY
- `timestamp`: TEXT (ISO 8601)
- `input`: TEXT (entrada del usuario)
- `output`: TEXT (respuesta del sistema)
- `intent`: TEXT (clasificaciÃ³n: CODIGO, CHAT, ANALISIS)

## ğŸ› SoluciÃ³n de Problemas

### "Ollama no parece estar instalado"
AsegÃºrate de que Ollama estÃ¡ instalado y corriendo:
```bash
ollama serve
```

### "Error al conectar con Ollama"
Verifica que el modelo estÃ¡ descargado:
```bash
ollama pull llama3.1
```

### Puerto 5001 en uso
Cambia el puerto en `bridge/server.py` o usa la variable de entorno `PORT`.

## ğŸ¯ PrÃ³ximos Pasos

- IntegraciÃ³n con interfaces web externas
- Soporte para mÃºltiples modelos
- Sistema de plugins expandible
- AnÃ¡lisis de cÃ³digo avanzado

## ğŸ“ Soporte

Para reportar problemas o contribuir, abre un issue en el repositorio.

---

**AMA-Intent v3** - Sistema de Inteligencia BiomimÃ©tica Local
