# ğŸ§  AMA-Intent v3: Cerebro Local BiomimÃ©tico

Sistema de inteligencia artificial biomimÃ©tica diseÃ±ado para ejecutar procesos de manera local, funcionando como "Cortex" de Qodeia.com. 

Esta versiÃ³n v3 representa una refactorizaciÃ³n completa hacia una arquitectura minimalista y funcional, reduciendo las dependencias en un 84% y simplificando la estructura en un 80%.

## ğŸš€ Funcionalidad
- **Local**: Corre completamente en tu mÃ¡quina usando Ollama con Llama 3.1
- **Inteligente**: Memoria SQLite persistente con clasificaciÃ³n de intenciÃ³n automÃ¡tica
- **Conectado**: HTTP API FastHTML para integraciÃ³n con aplicaciones externas
- **Seguro**: EjecuciÃ³n localhost por defecto, sin exposiciÃ³n a internet

## ğŸ“ Estructura del Proyecto

```plaintext
proyecto-ama-intent/
â”œâ”€â”€ .env                  # (NO SUBIR A GITHUB) Claves y secretos
â”œâ”€â”€ .gitignore            # Importante: para ignorar .env y __pycache__
â”œâ”€â”€ README.md             # El manual de uso biomimÃ©tico
â”œâ”€â”€ requirements.txt      # Dependencias ligeras
â”œâ”€â”€ start.py              # El Ãºnico archivo que necesitas ejecutar
â”œâ”€â”€ data/                 # Donde vive tu memoria (SQLite)
â”‚   â””â”€â”€ ama_memory.db
â”œâ”€â”€ local_cortex/         # ğŸ§  LÃ“GICA PURA (Tu cerebro local)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ thought.py        # Procesa texto con Llama 3
â”‚   â””â”€â”€ memory.py         # Gestiona recuerdos en SQLite
â””â”€â”€ bridge/               # ğŸŒ‰ CONEXIÃ“N (Servidor Web)
    â”œâ”€â”€ __init__.py
    â””â”€â”€ server.py         # API FastHTML que habla con Qodeia.com
```

## ğŸ› ï¸ InstalaciÃ³n

### Requisitos Previos
1. Python 3.8 o superior
2. Ollama instalado y corriendo

### Pasos de InstalaciÃ³n

```bash
# 1. Clonar el repositorio
git clone https://github.com/dgr198213-ui/proyecto-ama-intent.git
cd proyecto-ama-intent

# 2. Instalar dependencias
pip install -r requirements.txt

# 3. Verificar que Ollama estÃ¡ corriendo
ollama serve  # En otra terminal

# 4. Descargar el modelo (si no lo tienes)
ollama pull llama3.1
```

## ğŸš€ Uso

### Iniciar el Sistema

```bash
python start.py
```

El sistema:
1. VerificarÃ¡ la carpeta `data/` (la crearÃ¡ si no existe)
2. VerificarÃ¡ que Ollama estÃ¡ disponible
3. IniciarÃ¡ el servidor en puerto 5001

### Acceder a la Interfaz

Abre tu navegador en: http://localhost:5001

### API Endpoint

**POST** `/api/synapse`

**ParÃ¡metros:**
- `input` (string): El texto a procesar

**Respuesta:**
```json
{
  "status": "success",
  "intent": "CHAT|CODIGO|ANALISIS",
  "response": "Respuesta generada por el modelo",
  "timestamp": "2026-01-23T16:35:20.123456"
}
```

## ğŸ§  Arquitectura

### Local Cortex (Cerebro Local)
- **thought.py**: Procesa entradas usando Llama 3.1 a travÃ©s de Ollama
  - `LocalBrain.think()`: Genera respuestas contextualizadas
  - `LocalBrain.fast_classify()`: Clasifica el tipo de solicitud

- **memory.py**: Gestiona la memoria persistente
  - `init_db()`: Inicializa la base de datos SQLite
  - `save_thought()`: Guarda interacciones
  - `get_last_thoughts()`: Recupera contexto reciente

### Bridge (Puente HTTP)
- **server.py**: API FastHTML que conecta con el mundo exterior
  - Endpoint `/`: Interfaz de monitoreo
  - Endpoint `/api/synapse`: Procesa solicitudes

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno (.env)

```bash
# Opcional: configurar modelo diferente
OLLAMA_MODEL=llama3.1

# Opcional: cambiar puerto
PORT=5001
```

## ğŸ“Š Base de Datos

El sistema usa SQLite para persistir interacciones:

**Tabla: interactions**
- `id`: INTEGER PRIMARY KEY
- `timestamp`: TEXT (ISO 8601)
- `input`: TEXT (entrada del usuario)
- `output`: TEXT (respuesta del sistema)
- `intent`: TEXT (clasificaciÃ³n: CODIGO, CHAT, ANALISIS)

## ğŸ› SoluciÃ³n de Problemas

### "Ollama no parece estar instalado"
AsegÃºrate de que Ollama estÃ¡ instalado y corriendo:
```bash
ollama serve
```

### "Error al conectar con Ollama"
Verifica que el modelo estÃ¡ descargado:
```bash
ollama pull llama3.1
```

### Puerto 5001 en uso
Cambia el puerto en `bridge/server.py` o usa la variable de entorno `PORT`:
```bash
PORT=5002 python start.py
```

### Error al importar mÃ³dulos
Si ves errores de importaciÃ³n, reinstala las dependencias:
```bash
pip install -r requirements.txt --force-reinstall
```

## ğŸ§ª Pruebas

Para verificar que todo funciona correctamente, ejecuta la suite de pruebas:

```bash
python test_ama_v3.py
```

Esta suite verifica:
- Estructura de directorios
- ImportaciÃ³n de mÃ³dulos
- Dependencias correctas
- Sintaxis de Python
- Funciones de memoria (init, save, retrieve)

## ğŸ“‹ Novedades en v3

### Cambios Principales desde v2
- âœ… **ReducciÃ³n de dependencias**: De 25+ paquetes a solo 4
- âœ… **SimplificaciÃ³n estructural**: De 15+ directorios a 3 mÃ³dulos core
- âœ… **CÃ³digo mÃ¡s limpio**: ~200 lÃ­neas vs ~10,000 lÃ­neas anteriores
- âœ… **Seguridad mejorada**: Localhost por defecto, context managers, subprocess seguro
- âœ… **Tests automatizados**: Suite completa con 7 tests (100% cobertura core)

### CaracterÃ­sticas Eliminadas
- âŒ Dashboard web complejo
- âŒ Sistema de plugins v2
- âŒ AutenticaciÃ³n multi-usuario
- âŒ IntegraciÃ³n MiniMax
- âŒ Soporte Docker (por ahora)

Ver `REFACTORING_SUMMARY.md` para detalles completos.

## ğŸ¯ PrÃ³ximos Pasos

- IntegraciÃ³n con interfaces web externas
- Soporte para mÃºltiples modelos
- Sistema de plugins expandible
- AnÃ¡lisis de cÃ³digo avanzado

## ğŸ“ Soporte

Para reportar problemas o contribuir, abre un issue en el repositorio.

---

**AMA-Intent v3** - Sistema de Inteligencia BiomimÃ©tica Local
